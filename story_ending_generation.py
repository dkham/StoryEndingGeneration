# -*- coding: utf-8 -*-
"""Story Ending Generation_266 Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uv_0oWxrgzSV1gLIN8ZHkh2eCcVQKRlG

#**Evaluating Model Approaches to Automated Story Ending Generation**
Melissa Hartwick, Daisy Khamphakdy, Tanmay Mahapatra

W266, MIDS, UC Berkeley

## Set-up
    1. Import tensorflow dataset and other prerequisites
"""

!pip install gensim==3.8.3 --quiet

!pip install transformers --quiet

!pip install -U tensorflow-text --quiet

!pip install pydot --quiet

import pandas as pd
import numpy as np
import os

import tensorflow as tf
from tensorflow import keras

from tensorflow.keras.layers import Embedding, LSTM, Input, Dense, Lambda, Dropout, Bidirectional
from tensorflow.keras.models import Model
import tensorflow.keras.backend as K
import tensorflow_datasets as tfds
import tensorflow_text as tf_text
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
import keras.utils as ku
from tensorflow.keras.optimizers import Adam
from keras.utils import pad_sequences
#For the OPT decoder-only model
from transformers import AutoTokenizer, TFOPTForCausalLM

import sklearn as sk
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt

import re
import random
from collections import Counter

#Check the version of TensorFlow you are using
print(tf.__version__)
print(tf.config.list_physical_devices('GPU'))

random.seed(10)

#BLEURT SCORES
# Set tf.enable_eager_execution() if using TF 1.x.
import tensorflow as tf
print(tf.__version__)

!pip install --upgrade pip

!pip install git+https://github.com/google-research/bleurt.git

import bleurt
from bleurt import score



# ROUGE SCORES
!pip install rouge
from rouge import Rouge

from google.colab import drive

drive.mount('/content/gdrive')

raw_df = pd.read_csv('/content/gdrive/My Drive/266 Final Project/ROCStories_winter2017 - ROCStories_winter2017.csv')

raw_df.head()

"""## Data Pre-processing
    1. Drop unnecessary columns, "storyid" and the final sentence we want to correctly generate, "sentence5".
"""

df = raw_df.drop(['storyid'], axis=1)

df.head()

"""##EDA
    Conduct basic EDA on dataset:
        1. Summary statistics
        2. Average length of sentences for all sentences
        3. Average length of sentences for sentence1 - sentence5 (or 4?)
        4. Remove duplicate rows
"""

print("----------Summary Stats-------------")
df.describe()

#Avg length of characters for sentence
print("----------Average Length of Sentence for Each Column-------------")
no_of_row=df.shape[0]
for col in df.sum():
    print(round(len(col)/no_of_row))

# df['Avg_length'] = df["sentence1"].apply(lambda x: np.mean([len(w) for w in x.split()]))
# df['Avg_length']

"""# Create X & Y variables


```
# X = Sentence 1 + Sentence 2 + Sentence 3 + Sentence 4
# Y = Sentence 5
```


"""

#Create x variable
cols = ['sentence1', 'sentence2', 'sentence3', 'sentence4']
df['X'] = df[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)

df

df['X'].describe()

df2 = df.drop([36997, 1])

df2

"""## Create DataFrame just with X  & Y"""

df3 = df2.drop(['storytitle','sentence1','sentence2','sentence3','sentence4'], axis=1)
df3

"""## Remove apostrophes from sentences to not affect strings being converted to tensors"""

df3.applymap(lambda x: x.replace('"', ''))

df3

"""## Train Test Split
    
    Dataframes:
        1. Train data (df_train) - 80%
        2. Test data (df_test) - 20%
"""

train_size, test_size = .8, .2
df_train, df_test = train_test_split(df3, train_size = train_size, random_state=0)

print('---------Train/Test Record Count--------')
print("df_train:", len(df_train))
print("df_test:", len(df_test))
print()
print('---------Train/Test Shape--------')
print("df_train shape:", df_train.shape)
print("df_test shape:", df_test.shape)

"""## Create X & Y train and test lists from df_train and df_test

```
After this step, X and Y variables in the train and test sets will be ready for tokenization
```
"""

#Convert x_train to a list from df_train

x_train = df_train['X'].astype(str).values.tolist()
len(x_train)

#Convert x_train to a list from df_train

x_test = df_test['X'].astype(str).values.tolist()
len(x_test)

#Convert 'sentence5' (our Y variable) in df_train to tensor

y_train = df_train['sentence5'].astype(str).values.tolist()
len(y_train)

#Convert 'sentence5' (our Y variable) in df_test to tensor

y_test = df_test['sentence5'].astype(str).values.tolist()
len(y_test)

"""# Neural Network Models
    Tokenize & Build text generation models.
    
    Models:
        1. Bidirectional LSTM (Bi-LSTM)
        2. Encoder/Decoder using BERT and GPT2
        3. Decoder only using OPT

#### Bi-LSTM

    Step 1) Create Baseline Bi-LSTM model with arbitrarily chosen hyperparameters
    
    Step 2) Hyperparameter Tuning for Bi-LSTM:
        2. Multilayered Bi-LSTM
        3. Increased parameters
        4. Decreased parameters
        5. Decreased learning rate
        6. Increased learning rate
    
    Step 3) Compare results in loss, accuracy, BLEURT, ROUGE, and manual inspection of generated text
"""

cols = ['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5']
data = df[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)

data.head()

#combine all in one cell.
data
separator = " "

# create list of words for each line
list_of_full_sentences = []
for i in data:
    list_of_full_sentences.append(i)

list_of_full_sentences[12]

result = [i for i in list_of_full_sentences if i.startswith('Miss Weeks ')]
print(result)
#print(list_of_full_sentences.index("Tom was making hamburger buns. But he noticed that his dough wasn't fermenting. He decided to put the dough in the oven anyway. And he was left with dense, doughy buns."))

cols = ['sentence1', 'sentence2', 'sentence3', 'sentence4']
input_sentences = df[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)

input_sentences.head()

#combine all in one cell.
separator = " "

# create list of words for each line
list_of_input_sentences = []
for i in input_sentences:
    list_of_input_sentences.append(i)

list_of_input_sentences[90]

## Maintain context sentences
x_train_sentences = x_train
x_test_sentences = x_test
y_train_sentences = y_train
y_test_sentences = y_test

## Explore Training dataset
corpus = df3['X']
t = Tokenizer(num_words=None, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', lower=True, split=' ', char_level=False, oov_token=None, document_count=0)
t.fit_on_texts(corpus)
print(t.word_counts)
print(t.word_docs)
print(t.document_count)
print(t.word_index)
print('Found %s unique tokens.' % len(t.word_index))

## Hyperparameters

## 1) Baseline
vocab_size = 10000
embedding_dim = 256 #length of vector for each word. I've also tried 64 and 1000. #You can pick a power of 32 (64, 128, 256) to speed up modeling training depending on your software framework and hardware. Or you just arbitrarily pick a power of 32 and hope to get some optimization effect.
max_length = 74 #Try 74 bc max_seq_length calculated is 74.
trunc_type = 'pre' # use pre as to not wash out and forget the ending
padding_type = 'pre'
oov_tok = '<OOV>'
n_units = 150

## 2) Increased Params
# vocab_size = 20000
# embedding_dim = 1000
# max_length = 74
# trunc_type = 'pre'
# padding_type = 'pre'
# oov_tok = '<OOV>'
# n_units = 300

## 3) Decreased Params
# vocab_size = 500
# embedding_dim = 50
# max_length = 74
# trunc_type = 'pre'
# padding_type = 'pre'
# oov_tok = '<OOV>'
# n_units = 50

## Tokenize & pad X
tokenizer = Tokenizer (num_words = vocab_size, oov_token = oov_tok)
tokenizer.fit_on_texts(x_train)

x_train = tokenizer.texts_to_sequences(x_train)
x_test = tokenizer.texts_to_sequences(x_test)

x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,
                                                        padding=padding_type,
                                                        truncating=trunc_type,
                                                        maxlen = max_length)

x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,
                                                        padding=padding_type,
                                                        truncating=trunc_type,
                                                        maxlen = max_length)

## Tokenize & pad Y
tokenizer = Tokenizer (num_words = vocab_size, oov_token = oov_tok)
tokenizer.fit_on_texts(y_train)

y_train = tokenizer.texts_to_sequences(y_train)
y_test = tokenizer.texts_to_sequences(y_test)

y_train = tf.keras.preprocessing.sequence.pad_sequences(y_train,
                                                        padding=padding_type,
                                                        truncating=trunc_type,
                                                        maxlen = max_length)

y_test = tf.keras.preprocessing.sequence.pad_sequences(y_test,
                                                        padding=padding_type,
                                                        truncating=trunc_type,
                                                        maxlen = max_length)

print(tokenizer.word_index)

"""##### 1) Baseline Bi-LSTM"""

## Baseline Bi-LSTM model
# BiLSTM can produce a more meaningful output, combining LSTM layers from both directions.
# BiLSTM will be slower

def create_bilstm_model(max_sequence_length, total_words, n_units):
  bilstm_model = Sequential()

  bilstm_model.add(Embedding(total_words, embedding_dim))

  # First bidirectional LSTM layer
  bilstm_model.add(Bidirectional(LSTM(n_units)))
  bilstm_model.add(Dropout(0.1))

  bilstm_model.add(Dense(total_words, activation='softmax'))

  bilstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])

  return bilstm_model

bilstm_model = create_bilstm_model(max_length, vocab_size, n_units)
bilstm_model.summary()

## Epochs & Batch Size

#use early stopping to save computing power
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)

epochs=20
batch_size=100

bilstm_model_history = bilstm_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), callbacks=[early_stop])

plt.figure(figsize=(5,3))
plt.title('Baseline Bi-LSTM Loss')
plt.plot(bilstm_model_history.history['loss'], label='train')
plt.plot(bilstm_model_history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.figure(figsize=(5,3))
plt.title('Baseline Bi-LSTM Accuracy')
plt.plot(bilstm_model_history.history['accuracy'], label='train')
plt.plot(bilstm_model_history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

def generate_text(model,tokenizer,text_seq_length,seed_text,n_words):
    text = []
    for i in range(n_words):
        encoded = tokenizer.texts_to_sequences([seed_text])[0]
        encoded = pad_sequences([encoded],maxlen = text_seq_length,truncating = 'pre')

        y_predict = np.argmax(model.predict(encoded),axis=1)

        predicted_word = " "

        for word,index in tokenizer.word_index.items():
            if np.any(index == y_predict):
                predicted_word = word
                break
        seed_text = seed_text + " " + predicted_word
        text.append(predicted_word)
    return " ".join(text)

sentence_idx = 2
input_text = x_train_sentences[sentence_idx]
print("Input text: ", input_text)

output_text = generate_text(bilstm_model,tokenizer,50,input_text,70)
print(output_text)

# Function to count repeated words
def repeated_words(text):
  reg = re.compile('\S{4,}')
  count_repeated_words = Counter(ma.group() for ma in reg.finditer(output_text))
  return print(count_repeated_words)

# Repeated words from baseline model
repeated_words(output_text)

"""Hyperparameter Tuning:
        1. Increase parameters
        2. Decrease parameters
        3. Create multiple bi-lstm layers

#####2) Multilayered Bi-LSTM
"""

# Multi_bilstm with baseline parameters

def create_multi_bilstm_model(max_sequence_length, total_words, n_units):
  bilstm_model = Sequential()

  # Embedding layer stores one vector per word. When called, it converts the sequences
  # of word indices into sequences of vectors. After training, words with similar meanings often have the similar vectors.
  bilstm_model.add(Embedding(total_words, embedding_dim))

  # First bidirectional LSTM layer
  bilstm_model.add(Bidirectional(LSTM(n_units, return_sequences=True))),
  # Second bidirectional LSTM layer
  bilstm_model.add(Bidirectional(LSTM(100))),
  bilstm_model.add(Dropout(0.1)) #dropout w/ probability of 10%

  bilstm_model.add(Dense(embedding_dim, activation='softmax'))

  bilstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])

  return bilstm_model

multi_bilstm_model = create_multi_bilstm_model(max_length, vocab_size, n_units)
multi_bilstm_model.summary()

multi_bilstm_history = multi_bilstm_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), callbacks=[early_stop])

plt.figure(figsize=(5,3))
plt.title('Multilayered Bi-LSTM Loss')
plt.plot(multi_bilstm_history.history['loss'], label='train')
plt.plot(multi_bilstm_history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.figure(figsize=(5,3))
plt.title('Multilayered Bi-LSTM Accuracy')
plt.plot(multi_bilstm_history.history['accuracy'], label='train')
plt.plot(multi_bilstm_history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

sentence_idx = 2
input_text = x_train_sentences[sentence_idx]
print("Input text: ", input_text)

output_text = generate_text(multi_bilstm_model,tokenizer,50,input_text,70)
print(output_text)

repeated_words(output_text)

"""#####3) Increased Parameters"""

increased_params_bilstm_model = create_bilstm_model(max_length, vocab_size, n_units)
increased_params_bilstm_model.summary()

increased_params_bilstm_model_history = increased_params_bilstm_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), callbacks=[early_stop])

plt.figure(figsize=(5,3))
plt.title('Bi-LSTM Loss: Increased Params')
plt.plot(increased_params_bilstm_model_history.history['loss'], label='train')
plt.plot(increased_params_bilstm_model_history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.figure(figsize=(5,3))
plt.title('Bi-LSTM Accuracy: Increased Params')
plt.plot(increased_params_bilstm_model_history.history['accuracy'], label='train')
plt.plot(increased_params_bilstm_model_history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

sentence_idx = 2
input_text = x_train_sentences[sentence_idx]
print("Input text: ", input_text)

output_text = generate_text(increased_params_bilstm_model,tokenizer,50,input_text,70)
print(output_text)

repeated_words(output_text)

"""#####4) Decreased Parameters"""

decreased_params_bilstm_model = create_bilstm_model(max_length, vocab_size, n_units)
decreased_params_bilstm_model.summary()

decreased_params_bilstm_model_history = decreased_params_bilstm_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), callbacks=[early_stop])

plt.figure(figsize=(5,3))
plt.title('Bi-LSTM Loss: Decreased Params')
plt.plot(decreased_params_bilstm_model_history.history['loss'], label='train')
plt.plot(decreased_params_bilstm_model_history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.figure(figsize=(5,3))
plt.title('Bi-LSTM Accuracy: Decreased Params')
plt.plot(decreased_params_bilstm_model_history.history['accuracy'], label='train')
plt.plot(decreased_params_bilstm_model_history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

sentence_idx = 2
input_text = x_train_sentences[sentence_idx]
print("Input text: ", input_text)

output_text = generate_text(decreased_params_bilstm_model,tokenizer,50,input_text,70)
print(output_text)

repeated_words(output_text)

"""#####5) Decreased Learning Rate"""

# Model using baseline params
def create_bilstm_model_lr(max_sequence_length, total_words, n_units):
  bilstm_model_lr = Sequential()

  bilstm_model_lr.add(Embedding(vocab_size, embedding_dim))

  # First bidirectional LSTM layer
  bilstm_model_lr.add(Bidirectional(LSTM(n_units)))
  bilstm_model_lr.add(Dropout(0.1))

  bilstm_model_lr.add(Dense(total_words, activation='softmax'))

  #adam default learning rate = 0.001
  #adam = Adam(lr=0.005) #decreased learning rate
  adam = Adam(lr=0.1) #increased learning rate

  bilstm_model_lr.compile(loss='categorical_crossentropy', optimizer=adam, metrics = ['accuracy'])

  return bilstm_model_lr

lr_bilstm_model = create_bilstm_model_lr(max_length, vocab_size, n_units)
lr_bilstm_model.summary()

lr_bilstm_history = lr_bilstm_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), callbacks=[early_stop])

plt.figure(figsize=(5,3))
plt.title('Bi-LSTM Loss: Decreased Learning Rate (0.005)')
plt.plot(lr_bilstm_history.history['loss'], label='train')
plt.plot(lr_bilstm_history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.figure(figsize=(5,3))
plt.title('Bi-LSTM Accuracy: Decreased Learning Rate (0.005)')
plt.plot(lr_bilstm_history.history['accuracy'], label='train')
plt.plot(lr_bilstm_history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

sentence_idx = 2
input_text = x_train_sentences[sentence_idx]
print("Input text: ", input_text)

output_text = generate_text(lr_bilstm_model,tokenizer,50,input_text,70)
print(output_text)

repeated_words(output_text)

"""#####6) Increased learning rate"""

increase_lr_bilstm_model = create_bilstm_model_lr(max_length, vocab_size, n_units)
increase_lr_bilstm_model.summary()

increase_lr_bilstm_history = increase_lr_bilstm_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), callbacks=[early_stop])

plt.figure(figsize=(5,3))
plt.title('Bi-LSTM Loss: Increased Learning Rate (0.1)')
plt.plot(increase_lr_bilstm_history.history['loss'], label='train')
plt.plot(increase_lr_bilstm_history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.figure(figsize=(5,3))
plt.title('Bi-LSTM Accuracy: Increased Learning Rate (0.1)')
plt.plot(increase_lr_bilstm_history.history['accuracy'], label='train')
plt.plot(increase_lr_bilstm_history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

sentence_idx = 2
input_text = x_train_sentences[sentence_idx]
print("Input text: ", input_text)

output_text = generate_text(increase_lr_bilstm_model,tokenizer,50,input_text,70)
print(output_text)

repeated_words(output_text)

"""#### BI-LSTM Evaluation for 6 Models"""

#sample_y_test = str(y_test[0:100])
sample_y_test = y_test_sentences[0:100]
print(len(sample_y_test))

seed_text = str(x_test_sentences[:100])
bilstm_generated_text = []
for i in range(100):
  #seed_text initial words to be passed for text gen
  encoded = tokenizer.texts_to_sequences([seed_text])[0]

  # pad sequences
  encoded = pad_sequences([encoded],maxlen = 50,truncating = 'pre')

  # model.predict returns the token id for the predicted word
  y_predict = np.argmax(increased_params_bilstm_model.predict(encoded),axis=1)

  #word_index converts back into a word using word_index
  predicted_word = " "
  for word,index in tokenizer.word_index.items():
    if np.any(index == y_predict):
      predicted_word = word
      break
  seed_text = seed_text + " " + predicted_word

  #appends to bilstm_generated_text
  bilstm_generated_text.append(predicted_word)
  " ".join(bilstm_generated_text)

## 1) BLEURT - Baseline BiLSTM
import bleurt
from bleurt import score

references = tf.constant(sample_y_test) #correct results
candidates = tf.constant(bilstm_generated_text) #generated 5th sentence

bleurt_ops = score.create_bleurt_ops()
bleurt_out = bleurt_ops(references=references, candidates=candidates)

assert bleurt_out["predictions"].shape == (100,)
print(bleurt_out["predictions"],'\n')
print("Model 1 Average BLEURT score: ", sum(bleurt_out["predictions"])/100)

## 1) ROUGE - Baseline Bi-LSTM

rouge = Rouge()

# Generate a set of sentences from your model
generated_sentences = bilstm_generated_text

# Provide a set of reference sentences to compare against
reference_sentences = sample_y_test

# Compute the ROUGE scores
scores = rouge.get_scores(generated_sentences, reference_sentences, avg=True)

# Print scores in a prettier format
print("Model 1 ROUGE scores")
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

## 2) BLEURT - Multilayered BiLSTM
import bleurt
from bleurt import score

references = tf.constant(sample_y_test) #correct results
candidates = tf.constant(bilstm_generated_text) #generated 5th sentence

bleurt_ops = score.create_bleurt_ops()
bleurt_out = bleurt_ops(references=references, candidates=candidates)

assert bleurt_out["predictions"].shape == (100,)
print(bleurt_out["predictions"],'\n')
print("Model 1 Average BLEURT score: ", sum(bleurt_out["predictions"])/100)

## 2) ROUGE - Multilayered Bi-LSTM

rouge = Rouge()

# Generate a set of sentences from your model
generated_sentences = bilstm_generated_text

# Provide a set of reference sentences to compare against
reference_sentences = sample_y_test

# Compute the ROUGE scores
scores = rouge.get_scores(generated_sentences, reference_sentences, avg=True)

# Print scores in a prettier format
print("Model 1 ROUGE scores")
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

## 3) BLEURT - Increased Hyperparameters
import bleurt
from bleurt import score

references = tf.constant(sample_y_test) #correct results
candidates = tf.constant(bilstm_generated_text) #generated 5th sentence

bleurt_ops = score.create_bleurt_ops()
bleurt_out = bleurt_ops(references=references, candidates=candidates)

assert bleurt_out["predictions"].shape == (100,)
print(bleurt_out["predictions"],'\n')
print("Model 1 Average BLEURT score: ", sum(bleurt_out["predictions"])/100)

## 3) ROUGE - Increased Hyperparameters

rouge = Rouge()

# Generate a set of sentences from your model
generated_sentences = bilstm_generated_text

# Provide a set of reference sentences to compare against
reference_sentences = sample_y_test

# Compute the ROUGE scores
scores = rouge.get_scores(generated_sentences, reference_sentences, avg=True)

# Print scores in a prettier format
print("Model 1 ROUGE scores")
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

## 4) BLEURT - Decreased Hyperparameters
import bleurt
from bleurt import score

references = tf.constant(sample_y_test) #correct results
candidates = tf.constant(bilstm_generated_text) #generated 5th sentence

bleurt_ops = score.create_bleurt_ops()
bleurt_out = bleurt_ops(references=references, candidates=candidates)

assert bleurt_out["predictions"].shape == (100,)
print(bleurt_out["predictions"],'\n')
print("Model 1 Average BLEURT score: ", sum(bleurt_out["predictions"])/100)

## 4) ROUGE - Decreased Hyperparameters

rouge = Rouge()

# Generate a set of sentences from your model
generated_sentences = bilstm_generated_text

# Provide a set of reference sentences to compare against
reference_sentences = sample_y_test

# Compute the ROUGE scores
scores = rouge.get_scores(generated_sentences, reference_sentences, avg=True)

# Print scores in a prettier format
print("Model 1 ROUGE scores")
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

## 5) BLEURT - Decreased Learning Rate
import bleurt
from bleurt import score

references = tf.constant(sample_y_test) #correct results
candidates = tf.constant(bilstm_generated_text) #generated 5th sentence

bleurt_ops = score.create_bleurt_ops()
bleurt_out = bleurt_ops(references=references, candidates=candidates)

assert bleurt_out["predictions"].shape == (100,)
print(bleurt_out["predictions"],'\n')
print("Model 1 Average BLEURT score: ", sum(bleurt_out["predictions"])/100)

## 5) ROUGE - Decreased Learning Rate

rouge = Rouge()

# Generate a set of sentences from your model
generated_sentences = bilstm_generated_text

# Provide a set of reference sentences to compare against
reference_sentences = sample_y_test

# Compute the ROUGE scores
scores = rouge.get_scores(generated_sentences, reference_sentences, avg=True)

# Print scores in a prettier format
print("Model 1 ROUGE scores")
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

## 6) BLEURT - Decreased Learning Rate
import bleurt
from bleurt import score

references = tf.constant(sample_y_test) #correct results
candidates = tf.constant(bilstm_generated_text) #generated 5th sentence

bleurt_ops = score.create_bleurt_ops()
bleurt_out = bleurt_ops(references=references, candidates=candidates)

assert bleurt_out["predictions"].shape == (100,)
print(bleurt_out["predictions"],'\n')
print("Model 1 Average BLEURT score: ", sum(bleurt_out["predictions"])/100)

## 6) ROUGE - Decreased Learning Rate

rouge = Rouge()

# Generate a set of sentences from your model
generated_sentences = bilstm_generated_text

# Provide a set of reference sentences to compare against
reference_sentences = sample_y_test

# Compute the ROUGE scores
scores = rouge.get_scores(generated_sentences, reference_sentences, avg=True)

# Print scores in a prettier format
print("Model 1 ROUGE scores")
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

"""Observations: Baseline model did the best.


*   Had a lower loss than increased params
*   Did not overfit w/ decreased params.
* Multi-layers Bilstm had similar results to original. More layers would be needed to make a difference, but that's not needed for this as it would require more computing power (for little gain)
*  Baseline also had the least repition

####Bi-LSTM Evaluation for Winner Model (500 test set)


*   BLEURT
*   ROUGE
"""

#sample_y_test = str(y_test[0:100])
sample_y_test = y_test_sentences[0:500]
print(len(sample_y_test))

seed_text = str(x_test_sentences[:500])
bilstm_generated_text = []
for i in range(500):
  #seed_text initial words to be passed for text gen
  encoded = tokenizer.texts_to_sequences([seed_text])[0]

  # pad sequences
  encoded = pad_sequences([encoded],maxlen = 50,truncating = 'pre')

  # model.predict returns the token id for the predicted word
  y_predict = np.argmax(bilstm_model.predict(encoded),axis=1)

  #word_index converts back into a word using word_index
  predicted_word = " "
  for word,index in tokenizer.word_index.items():
    if np.any(index == y_predict):
      predicted_word = word
      break
  seed_text = seed_text + " " + predicted_word

  #appends to text
  bilstm_generated_text.append(predicted_word)
  " ".join(bilstm_generated_text)

## BLEURT
import bleurt
from bleurt import score

references = tf.constant(sample_y_test) #correct results
candidates = tf.constant(bilstm_generated_text) #generated 5th sentence

bleurt_ops = score.create_bleurt_ops()
bleurt_out = bleurt_ops(references=references, candidates=candidates)

assert bleurt_out["predictions"].shape == (500,)
print(bleurt_out["predictions"],'\n')
print("Model 1 Average BLEURT score: ", sum(bleurt_out["predictions"])/500)

## ROUGE

rouge = Rouge()

# Generate a set of sentences from your model
generated_sentences = bilstm_generated_text

# Provide a set of reference sentences to compare against
reference_sentences = sample_y_test

# Compute the ROUGE scores
scores = rouge.get_scores(generated_sentences, reference_sentences, avg=True)

# Print scores in a prettier format
print("Model 1 ROUGE scores")
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

"""#### BERT Encoder

    Create BERT model with standard hyperparameters
    
    Hyperparameter Tuning:
        1. Standar Hyperparameters
        2. Dropout, Weight decay
        3. Beam search
        4. Top-p
    
    Compare BLEURT/ROUGE scores
"""

x_train_sub = df_train['X'].astype(str).head(5000).values.tolist()
y_train_sub = df_train['sentence5'].astype(str).head(5000).values.tolist()
df_train_sub = df_train.head(5000)

x_test_sub = df_test['X'].astype(str).head(100).values.tolist()
y_test_sub = df_test['sentence5'].astype(str).head(100).values.tolist()
df_test_sub = df_test.head(100)

from transformers import BertTokenizer, BertForMaskedLM, AdamW

bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

bert_model = bert_model.to(device)

def generate_sentence(prompt, ending, tokenizer, model, device):
    # Encode the prompt using the tokenizer
    inputs = bert_tokenizer.encode(prompt, return_tensors='pt').to(device)

    # Get the masked token index
    masked_index = torch.where(inputs == bert_tokenizer.mask_token_id)[1]

    # Generate the next sentence
    output = bert_model.generate(inputs, max_length=75, do_sample=True)

    output_sentence = bert_tokenizer.decode(output[0], skip_special_tokens=True)

    return output_sentence

import csv

# Open a CSV file to write the output sentences
with open('bert-output.csv', mode='w', newline='') as csv_file:
    writer = csv.writer(csv_file)

    # Write the header row
    writer.writerow(['Prompt', 'Ending', 'Output'])

    # generate outputs for the test prompts and endings
    for prompt, ending in zip(x_test_sub, y_test_sub):
        output_sentence = generate_sentence(prompt, ending, bert_tokenizer, bert_model, device)
        # Print the generated sentence
        # print(prompt)
        # print(ending)
        # print(output_sentence)
        # print()

        # Write the output sentence to the CSV file
        writer.writerow([prompt, ending, output_sentence])

csv_file.close()

# Create a new list to hold the masked input sentences
masked_sentences = []

# Loop over each input sentence and add [MASK] to the end
for sentence in x_train_sub:
    masked_sentence = sentence + ' [MASK]'
    masked_sentences.append(masked_sentence)

    # Print the original and masked sentences for comparison
    # print(f'Original sentence: {sentence}')
    # print(f'Masked sentence:   {masked_sentence}\n')

# Fine-tuning pass#1 - Model learns using surrounding context

from tqdm import tqdm

device = torch.device("cuda")
bert_model = bert_model.to(device)

def fine_tune_bert(
    x_train, y_train,
    bert_model,
    bert_tokenizer,
    num_epochs=2,
    batch_size=8,
    learning_rate=2e-5):

    # Set the model to training mode
    bert_model.train()

    # Define the optimizer
    optimizer = AdamW(bert_model.parameters(), lr=learning_rate)

    # Loop through each epoch
    for epoch in range(num_epochs):

      epoch_loss = 0

      # Wrap the training data in a tqdm progress bar
      with tqdm(total=len(x_train)) as progress_bar:

        # Loop through each training example
        for i in range(len(x_train)):
            # Encode the input and output sequences using the tokenizer
            inputs = bert_tokenizer.encode(x_train[i], return_tensors='pt').to(device)
            labels = bert_tokenizer.encode(y_train[i], return_tensors='pt').to(device)

            # Get the masked token index
            masked_index = torch.where(inputs == bert_tokenizer.mask_token_id)[1]

            # Replace the masked token with the corresponding label token
            # inputs[0, masked_index] = labels[0, masked_index]

            optimizer.zero_grad()

            # Feed the inputs into the model,
            # labels are the inputs themselves
            # so the model learns from surrounding context
            outputs = bert_model(inputs, labels=inputs)

            loss = outputs.loss

            # Backpropagate the loss
            loss.backward()

            # Update the weights
            optimizer.step()

            progress_bar.update(1)

        # Calculate the average loss for the epoch
        avg_loss = epoch_loss / len(x_train)

        print(f"Epoch {epoch+1}: Loss = {loss.item():.4f}")

fine_tune_bert(masked_sentences, y_train_sub, bert_model, bert_tokenizer)

# Open a CSV file to write the output sentences
with open('bert-output-pass1.csv', mode='w', newline='') as csv_file:
    writer = csv.writer(csv_file)

    # Write the header row
    writer.writerow(['Prompt', 'Ending', 'Output'])

    # generate outputs for the test prompts and endings
    for prompt, ending in zip(x_test_sub, y_test_sub):
        output_sentence = generate_sentence(prompt, ending, bert_tokenizer, bert_model, device)
        # Print the generated sentence
        # print(prompt)
        # print(ending)
        # print(output_sentence)
        # print()

        # Write the output sentence to the CSV file
        writer.writerow([prompt, ending, output_sentence])

csv_file.close()

# Fine-tuning pass#2 - dropout, weight_decay; training without labels. Model learns using surrounding context
# bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')

# device = torch.device("cpu")
bert_model = bert_model.to(device)

def fine_tune_bert(
    x_train,
    y_train,
    bert_model,
    bert_tokenizer,
    num_epochs=3,
    batch_size=8,
    learning_rate=2e-5,
    dropout=0.2,
    weight_decay=0.01):

    # Define the BERT model with dropout
    bert_model.config.attention_probs_dropout_prob = dropout
    bert_model.config.hidden_dropout_prob = dropout

    bert_model.train()

    optimizer = AdamW(bert_model.parameters(), lr=learning_rate)

    for epoch in range(num_epochs):
      epoch_loss = 0

      # Wrap the training data in a tqdm progress bar
      with tqdm(total=len(x_train)) as progress_bar:
        # Loop through each training example
        for i in range(len(x_train)):
            # Encode the input and output sequences using the tokenizer
            inputs = bert_tokenizer.encode(x_train[i], return_tensors='pt').to(device)
            labels = bert_tokenizer.encode(y_train[i], return_tensors='pt').to(device)

            # Get the masked token index
            masked_index = torch.where(inputs == bert_tokenizer.mask_token_id)[1]

            optimizer.zero_grad()

            # Feed the inputs into the model,
            # labels are the inputs themselves
            # so the model learns from surrounding context
            outputs = bert_model(inputs, labels=inputs)

            loss = outputs.loss

            # Backpropagate the loss
            loss.backward()

            # Update the weights
            optimizer.step()

            # Add the loss for the current example to the epoch loss
            epoch_loss += loss.item()

            progress_bar.update(1)

        # Calculate the average loss for the epoch
        avg_loss = epoch_loss / len(x_train)

        print(f"Epoch {epoch+1}: Loss = {loss.item():.4f}")

fine_tune_bert(masked_sentences, y_train_sub, bert_model, bert_tokenizer)

import csv

bert_model.eval()

# Open a CSV file to write the output sentences
with open('bert-output-pass2.csv', mode='w', newline='') as csv_file:
    writer = csv.writer(csv_file)

    # Write the header row
    writer.writerow(['Prompt', 'Ending', 'Output'])

    # generate outputs for the test prompts and endings
    for prompt, ending in zip(x_test_sub, y_test_sub):
        output_sentence = generate_sentence(prompt, ending, bert_tokenizer, bert_model, device)
        # Print the generated sentence
        # print(prompt)
        # print(ending)
        # print(output_sentence)
        # print()

        # Write the output sentence to the CSV file
        writer.writerow([prompt, ending, output_sentence])

csv_file.close()

"""Defining a generate_text_beam_search function that takes a prompt string as input and generates text using beam search with a maximum sequence length of max_length, num_beams beams, and a temperature value of temperature. The generate method of the BERT model takes care of the beam search decoding process, so we don't need to modify the model or decoding process explicitly."""

# Generate using beam search

# Define the function to generate text using beam search
def generate_sentence_beam_search(prompt, bert_tokenizer, bert_model, device, max_length=75, num_beams=3, temperature=1.0):

    bert_model.eval()
    input_ids = bert_tokenizer.encode(prompt, add_special_tokens=True, return_tensors='pt').to(device)

    # Generate text using beam search
    outputs = bert_model.generate(input_ids=input_ids, max_length=max_length, num_beams=num_beams, temperature=temperature)
    text = bert_tokenizer.decode(outputs[0], skip_special_tokens=True)

    return text

# Open a CSV file to write the output sentences
with open('bert-output-beam-search.csv', mode='w', newline='') as csv_file:
    writer = csv.writer(csv_file)

    # Write the header row
    writer.writerow(['Prompt', 'Ending', 'Output'])

    # generate outputs for the test prompts and endings
    for prompt, ending in zip(x_test_sub, y_test_sub):
        output_sentence = generate_sentence_beam_search(prompt, bert_tokenizer, bert_model, device)
        # Print the generated sentence
        # print(prompt)
        # print(ending)
        # print(output_sentence)
        # print()

        # Write the output sentence to the CSV file
        writer.writerow([prompt, ending, output_sentence])

csv_file.close()

"""Defining the generate_text_top_p function that takes a prompt string as input and generates text using top-p sampling with a maximum sequence length of max_length, a top_p value of 0.9, and a temperature value of temperature."""

# Generate using top-p

# Define the function to generate text using top-p sampling
def generate_sentence_top_p(prompt, bert_tokenizer, bert_model, device, max_length=75, top_p=0.9, temperature=1.0):
    # Set the model to evaluation mode
    bert_model.eval()
    input_ids = bert_tokenizer.encode(prompt, add_special_tokens=True, return_tensors='pt').to(device)

    # Generate text using top-p sampling
    outputs = bert_model.generate(input_ids=input_ids, max_length=max_length, do_sample=True, top_p=top_p, temperature=temperature)

    text = bert_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return text

# Open a CSV file to write the output sentences
with open('bert-output-top-p.csv', mode='w', newline='') as csv_file:
    writer = csv.writer(csv_file)

    # Write the header row
    writer.writerow(['Prompt', 'Ending', 'Output'])

    # generate outputs for the test prompts and endings
    for prompt, ending in zip(x_test_sub, y_test_sub):
        output_sentence = generate_sentence_top_p(prompt, bert_tokenizer, bert_model, device)
        # Print the generated sentence
        # print(prompt)
        # print(ending)
        # print(output_sentence)
        # print()

        # Write the output sentence to the CSV file
        writer.writerow([prompt, ending, output_sentence])

csv_file.close()

"""#### BERT Model Evaluation:
- BLEURT
- ROUGE
"""

!pip install git+https://github.com/google-research/bleurt.git

import bleurt
from bleurt import score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bleurt_scorer = score.BleurtScorer()

# Evaluate the model on the validation set
scores = []
# for input_text, target_text in zip(x_val, y_val):
for prompt, ending in zip(x_test_sub, y_test_sub):
    # Generate the story ending
    output_sentence = generate_sentence(prompt, ending, bert_tokenizer, bert_model, device)
    # Print input and output for debugging
    # print(f"Input: {input_sentence}")
    # print(f"Output: {output_sentence}\n")

    # Compute the BLEURT score
    bleurt_score = bleurt_scorer.score(references=[output_sentence], candidates=[ending])[0]
    scores.append(bleurt_score)

# Compute the average BLEURT score
avg_bleurt_score = sum(scores) / len(scores)
print("Average BLEURT score:", avg_bleurt_score)

import bleurt
from bleurt import score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bleurt_scorer = score.BleurtScorer()

# Evaluate the model on the validation set
scores = []

for prompt, ending in zip(x_test_sub, y_test_sub):
    # Generate the story ending
    output_sentence = generate_sentence_beam_search(prompt, bert_tokenizer, bert_model, device)
    # Print input and output for debugging
    # print(f"Input: {input_sentence}")
    # print(f"Output: {output_sentence}\n")

    # Compute the BLEURT score
    bleurt_score = bleurt_scorer.score(references=[output_sentence], candidates=[ending])[0]
    scores.append(bleurt_score)

# Compute the average BLEURT score
avg_bleurt_score = sum(scores) / len(scores)
print("Average BLEURT score:", avg_bleurt_score)

import bleurt
from bleurt import score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bleurt_scorer = score.BleurtScorer()

# Evaluate the model on the validation set
scores = []

for prompt, ending in zip(x_test_sub, y_test_sub):
    # Generate the story ending
    output_sentence = generate_sentence_top_p(prompt, bert_tokenizer, bert_model, device)

    # Print input and output for debugging
    # print(f"Input: {input_sentence}")
    # print(f"Output: {output_sentence}\n")

    # Compute the BLEURT score
    bleurt_score = bleurt_scorer.score(references=[output_sentence], candidates=[ending])[0]
    scores.append(bleurt_score)

# Compute the average BLEURT score
avg_bleurt_score = sum(scores) / len(scores)
print("Average BLEURT score:", avg_bleurt_score)

!pip install rouge

from rouge import Rouge
rouge = Rouge()

# Define lists to store references and hypothesis
references = []
hypotheses = []

# Generate story endings for the test dataset using the BERT model
for prompt, ending in zip(x_test_sub, y_test_sub):
    # Generate the story ending
    output_sentence = generate_sentence(prompt, ending, bert_tokenizer, bert_model, device)

    references.append(ending)
    hypotheses.append(output_sentence)

    # Print input and output for debugging
    # print(f"Input: {input_text}")
    # print(f"Output: {output_text}\n")

# Evaluate using ROUGE
scores = rouge.get_scores(hypotheses, references, avg=True)
print(scores)
print()

# Print scores in a prettier format
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

from rouge import Rouge
rouge = Rouge()

# Define lists to store references and hypothesis
references = []
hypotheses = []

# Generate story endings for the test dataset using the BERT model
for prompt, ending in zip(x_test_sub, y_test_sub):
    # Generate the story ending
    output_sentence = generate_sentence_beam_search(prompt, bert_tokenizer, bert_model, device)

    references.append(ending)
    hypotheses.append(output_sentence)

    # Print input and output for debugging
    # print(f"Input: {input_text}")
    # print(f"Output: {output_text}\n")

# Evaluate using ROUGE
scores = rouge.get_scores(hypotheses, references, avg=True)
print(scores)
print()

# Print scores in a prettier format
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

from rouge import Rouge
rouge = Rouge()

# Define lists to store references and hypothesis
references = []
hypotheses = []

# Generate story endings for the test dataset using the BERT model
for prompt, ending in zip(x_test_sub, y_test_sub):
    # Generate the story ending
    output_sentence = generate_sentence_top_p(prompt, bert_tokenizer, bert_model, device)

    references.append(ending)
    hypotheses.append(output_sentence)

    # Print input and output for debugging
    # print(f"Input: {input_text}")
    # print(f"Output: {output_text}\n")

# Evaluate using ROUGE
scores = rouge.get_scores(hypotheses, references, avg=True)
print(scores)
print()

# Print scores in a prettier format
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

"""#### T5 - Text-to-Text Transfer Transformer

    Create T5 model with standard hyperparameters
    
    Hyperparameter Tuning:
        1. Standard Hyperparameters
        2. Fewer epochs, increased batch size
        3. SGD optimizer
        4. Dropout
    
    Compare BLEURT/ROUGE scores
"""

x_train_sub = df_train['X'].astype(str).head(5000).values.tolist()
y_train_sub = df_train['sentence5'].astype(str).head(5000).values.tolist()
df_train_sub = df_train.head(5000)

x_test_sub = df_test['X'].astype(str).head(100).values.tolist()
y_test_sub = df_test['sentence5'].astype(str).head(100).values.tolist()
df_test_sub = df_train.head(100)

# !pip install sentencepiece

# !pip uninstall transformers

# !pip install transformers[t5]

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
t5_tokenizer = T5Tokenizer.from_pretrained("t5-small")
t5_small_model = T5ForConditionalGeneration.from_pretrained("t5-small").to(device)

import csv

# Open a CSV file to write the output sentences
with open('t5-output.csv', mode='w', newline='') as csv_file:
    writer = csv.writer(csv_file)

    # Write the header row
    writer.writerow(['Prompt', 'Output'])

    # Generate story endings
    for input in x_test_sub:
        input_text = "Complete the story: " + input
        # Generate the story ending
        input_ids = t5_tokenizer.encode(input_text, return_tensors="pt").to(device)
        output_ids = t5_small_model.generate(input_ids=input_ids, max_length=75, num_beams=5, early_stopping=True)
        output_text = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)
        # Print the input and generated output
        # print("Input Text: " + input_text)
        # print("Generated Ending: " + output_text)
        # print()

        # Write the output sentence to the CSV file
        writer.writerow([input_text, output_text])

csv_file.close()

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW
from torch.utils.data import Dataset, DataLoader
from tqdm.auto import tqdm

# custom dataset for x_train and y_train data
class StoryDataset(Dataset):
    def __init__(self, x_train, y_train, t5_tokenizer):
        self.input_ids = []
        self.attention_masks = []
        self.labels = []
        for i in range(len(x_train_sub)):
            input_str = "Complete the story: " + x_train_sub[i]
            label_str = y_train[i]
            input_encodings_dict = t5_tokenizer(input_str, truncation=True, max_length=512, padding="max_length")
            label_encodings_dict = t5_tokenizer(label_str, truncation=True, max_length=512, padding="max_length")
            self.input_ids.append(input_encodings_dict['input_ids'])
            self.attention_masks.append(input_encodings_dict['attention_mask'])
            self.labels.append(label_encodings_dict['input_ids'])

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {'input_ids': torch.tensor(self.input_ids[idx]),
                'attention_mask': torch.tensor(self.attention_masks[idx]),
                'labels': torch.tensor(self.labels[idx])}

# Fine-tuning pass#1

# Define training parameters
model_name = 't5-small'
batch_size = 8
learning_rate = 3e-5
epochs = 3

# training dataset and dataloader
train_dataset = StoryDataset(x_train_sub, y_train_sub, t5_tokenizer)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

optimizer = AdamW(t5_small_model.parameters(), lr=learning_rate)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)

# Train
t5_small_model.train()
for epoch in range(epochs):
    epoch_loss = 0
    for batch in tqdm(train_dataloader, desc=f"Epoch {epoch}"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = t5_small_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs[0]
        epoch_loss += loss.item()
        loss.backward()
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
    print(f"Epoch {epoch} loss: {epoch_loss/len(train_dataloader):.4f}")

import csv

# Open a CSV file to write the output sentences
with open('t5-output-pass1.csv', mode='w', newline='') as csv_file:
    writer = csv.writer(csv_file)

    # Write the header row
    writer.writerow(['Prompt', 'Output'])

    # Generate story endings
    for input_sentence, actual_ending in zip(x_test_sub, y_test_sub):
        # Generate the story ending
        input_text = 'Complete the story: ' + input_sentence
        input_ids = t5_tokenizer.encode(input_text, return_tensors='pt').to(device)
        outputs = t5_small_model.generate(input_ids=input_ids, max_length=75, num_beams=5, early_stopping=True)
        generated_ending = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
        # Split the output text into sentences and pick the first sentence
        output_sentence = generated_ending.split(".")[0]

        # Print the input sentence, actual ending, and generated ending
        # print('Input sentence:', input_sentence)
        # print('Actual ending:', actual_ending)
        # print('Generated ending:', output_sentence)
        # print()

        # Write the output sentence to the CSV file
        writer.writerow([input_sentence, output_sentence])

csv_file.close()

# Fine-tune pass#1 - freeze layers, fewer epochs, higher lr, increased batch size crashed the GPU (not enough memory)

t5_small_model_epoch = T5ForConditionalGeneration.from_pretrained("t5-small").to(device)

n_layers = t5_small_model_lr.config.num_layers
print(f"The T5 model has {n_layers} layers")

# freeze the parameters of the first 3 decoder layers
n_layers_to_freeze = 3
for param in t5_small_model_lr.parameters():
    param.requires_grad = False

for i, module in enumerate(t5_small_model_lr.encoder.block):
    if i >= n_layers_to_freeze:
        for param in module.parameters():
            param.requires_grad = True

batch_size = 8
learning_rate = 4e-5
epochs =2

# training dataset and dataloader
train_dataset = StoryDataset(x_train_sub, y_train_sub, t5_tokenizer)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

optimizer = AdamW(t5_small_model_epoch.parameters(), lr=learning_rate)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)

# Train
t5_small_model_epoch.train()
for epoch in range(epochs):
    epoch_loss = 0
    for batch in tqdm(train_dataloader, desc=f"Epoch {epoch}"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = t5_small_model_epoch(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs[0]
        epoch_loss += loss.item()
        loss.backward()
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
    print(f"Epoch {epoch} loss: {epoch_loss/len(train_dataloader):.4f}")

# Learning rate finder
import math

optimizer = AdamW(t5_small_model.parameters(), lr=1e-7)
min_lr = 1e-7
max_lr = 1e-2
num_steps = len(train_dataloader) * epochs

# learning rate scheduler
lr_lambda = lambda step: math.exp((math.log(max_lr / min_lr) / num_steps) * step)
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

# gradually increasing the learning rate
t5_small_model.train()
losses = []
lrs = []
for epoch in range(epochs):
    epoch_loss = 0
    for i, batch in enumerate(tqdm(train_dataloader, desc=f"Epoch {epoch}")):
        # learning rate for the current batch
        lr = optimizer.param_groups[0]['lr']
        lrs.append(lr)

        # Train on the current batch
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = t5_small_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs[0]
        epoch_loss += loss.item()
        loss.backward()
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

        # loss for the current batch
        losses.append(loss.item())

# Plot the learning rate against the loss
import matplotlib.pyplot as plt
plt.plot(lrs, losses)
plt.xscale('log')
plt.xlabel('Learning rate')
plt.ylabel('Loss')
plt.show()

# Open a CSV file to write the output sentences
import csv

with open('t5-output-epochs.csv', mode='w', newline='') as csv_file:
    writer = csv.writer(csv_file)

    # Write the header row
    writer.writerow(['Prompt', 'Output'])

    # Generate story endings
    # for i in range(10):
    for input_sentence, actual_ending in zip(x_test_sub, y_test_sub):
        # input_sentence = x_test_sub[i]
        # actual_ending = y_test_sub[i]

        # Generate the story ending
        input_text = 'Complete the story: ' + input_sentence
        input_ids = t5_tokenizer.encode(input_text, return_tensors='pt').to(device)
        outputs = t5_small_model_epoch.generate(input_ids=input_ids, max_length=75, num_beams=5, early_stopping=True)
        generated_ending = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
        # Split the output text into sentences and pick the first sentence
        output_sentence = generated_ending.split(".")[0]

        # Print the input sentence, actual ending, and generated ending
        # print('Input sentence:', input_sentence)
        # print('Actual ending:', actual_ending)
        # print('Generated ending:', output_sentence)
        # print()

        # Write the output sentence to the CSV file
        writer.writerow([input_sentence, output_sentence])

csv_file.close()

# Fine-tune pass#2 - SGD optimizer

t5_small_model_sgd = T5ForConditionalGeneration.from_pretrained("t5-small").to(device)

batch_size = 8
learning_rate = 3e-5
epochs = 2

# training dataset and dataloader
train_dataset = StoryDataset(x_train_sub, y_train_sub, t5_tokenizer)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

optimizer = torch.optim.SGD(t5_small_model_sgd.parameters(), lr=learning_rate)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)

# Train
t5_small_model_sgd.train()
for epoch in range(epochs):
    epoch_loss = 0
    for batch in tqdm(train_dataloader, desc=f"Epoch {epoch}"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = t5_small_model_sgd(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs[0]
        epoch_loss += loss.item()
        loss.backward()
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
    print(f"Epoch {epoch} loss: {epoch_loss/len(train_dataloader):.4f}")

# Open a CSV file to write the output sentences
with open('t5-output-sgd.csv', mode='w', newline='') as csv_file:
    writer = csv.writer(csv_file)

    # Write the header row
    writer.writerow(['Prompt', 'Output'])

    # Generate story endings
    # for i in range(10):
    #     input_sentence = x_test_sub[i]
    #     actual_ending = y_test_sub[i]
    for input_sentence, actual_ending in zip(x_test_sub, y_test_sub):

        # Generate the story ending
        input_text = 'Complete the story: ' + input_sentence
        input_ids = t5_tokenizer.encode(input_text, return_tensors='pt').to(device)
        outputs = t5_small_model_sgd.generate(input_ids=input_ids, max_length=75, num_beams=5, early_stopping=True)
        generated_ending = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
        # Split the output text into sentences and pick the first sentence
        output_sentence = generated_ending.split(".")[0]

        # Print the input sentence, actual ending, and generated ending
        # print('Input sentence:', input_sentence)
        # print('Actual ending:', actual_ending)
        # print('Generated ending:', output_sentence)
        # print()

        # Write the output sentence to the CSV file
        writer.writerow([input_sentence, output_sentence])

csv_file.close()

# Fine-tune pass#3 - Dropout

from transformers import T5Config
from torch.nn import Dropout2d, Dropout

batch_size = 8
learning_rate = 3e-5
epochs = 2
dropout_rate = 0.2

t5_config = T5Config.from_pretrained('t5-small', dropout_rate=dropout_rate)
t5_small_dropout_model = T5ForConditionalGeneration.from_pretrained('t5-small', config=t5_config).to(device)

# training dataset and dataloader
train_dataset = StoryDataset(x_train_sub, y_train_sub, t5_tokenizer)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

optimizer = torch.optim.SGD(t5_small_dropout_model.parameters(), lr=learning_rate)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)

# Train
t5_small_dropout_model.train()
for epoch in range(epochs):
    epoch_loss = 0
    for batch in tqdm(train_dataloader, desc=f"Epoch {epoch}"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = t5_small_dropout_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs[0]
        epoch_loss += loss.item()
        loss.backward()
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
    print(f"Epoch {epoch} loss: {epoch_loss/len(train_dataloader):.4f}")

# Open a CSV file to write the output sentences
with open('t5-output-dropout.csv', mode='w', newline='') as csv_file:
    writer = csv.writer(csv_file)

    # Write the header row
    writer.writerow(['Prompt', 'Output'])

    # Generate story endings
    # for i in range(10):
    #     input_sentence = x_test_sub[i]
    #     actual_ending = y_test_sub[i]
    for input_sentence, actual_ending in zip(x_test_sub, y_test_sub):

        # Generate the story ending
        input_text = 'Complete the story: ' + input_sentence
        input_ids = t5_tokenizer.encode(input_text, return_tensors='pt').to(device)
        outputs = t5_small_dropout_model.generate(input_ids=input_ids, max_length=75, num_beams=5, early_stopping=True)
        generated_ending = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
        # Split the output text into sentences and pick the first sentence
        output_sentence = generated_ending.split(".")[0]

        # Print the input sentence, actual ending, and generated ending
        # print('Input sentence:', input_sentence)
        # print('Actual ending:', actual_ending)
        # print('Generated ending:', output_sentence)
        # print()

        # Write the output sentence to the CSV file
        writer.writerow([input_sentence, output_sentence])

csv_file.close()

"""#### T5 Model Evaluation:
- BLEURT
- ROUGE
"""

!pip install git+https://github.com/google-research/bleurt.git

import torch
import bleurt
from bleurt import score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bleurt_scorer = score.BleurtScorer()

# Evaluate the model on the validation set
scores = []

# Generate story endings
for input_sentence, actual_ending in zip(x_test_sub, y_test_sub):

    # Generate the story ending
    input_text = 'Complete the story: ' + input_sentence
    input_ids = t5_tokenizer.encode(input_text, return_tensors='pt').to(device)
    outputs = t5_small_model.generate(input_ids=input_ids, max_length=75, num_beams=5, early_stopping=True)
    generated_ending = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Split the output text into sentences and pick the first sentence
    output_sentence = generated_ending.split(".")[0]

    # Compute the BLEURT score
    bleurt_score = bleurt_scorer.score(references=[output_sentence], candidates=[actual_ending])[0]
    scores.append(bleurt_score)

    # Print the input sentence, actual ending, and generated ending
    # print('Input sentence:', input_sentence)
    # print('Actual ending:', actual_ending)
    # print('Generated ending:', output_sentence)
    # print()

# Compute the average BLEURT score
avg_bleurt_score = sum(scores) / len(scores)
print("Average BLEURT score:", avg_bleurt_score)

# Evaluate the model on the validation set
scores = []

# Generate story endings
for input_sentence, actual_ending in zip(x_test_sub, y_test_sub):

    # Generate the story ending
    input_text = 'Complete the story: ' + input_sentence
    input_ids = t5_tokenizer.encode(input_text, return_tensors='pt').to(device)
    outputs = t5_small_model_sgd.generate(input_ids=input_ids, max_length=75, num_beams=5, early_stopping=True)
    generated_ending = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Split the output text into sentences and pick the first sentence
    output_sentence = generated_ending.split(".")[0]

    # Compute the BLEURT score
    bleurt_score = bleurt_scorer.score(references=[output_sentence], candidates=[actual_ending])[0]
    scores.append(bleurt_score)

    # Print the input sentence, actual ending, and generated ending
    # print('Input sentence:', input_sentence)
    # print('Actual ending:', actual_ending)
    # print('Generated ending:', output_sentence)
    # print()

# Compute the average BLEURT score
avg_bleurt_score = sum(scores) / len(scores)
print("Average BLEURT score:", avg_bleurt_score)

# Evaluate the model on the validation set
scores = []

# Generate story endings
for input_sentence, actual_ending in zip(x_test_sub, y_test_sub):

    # Generate the story ending
    input_text = 'Complete the story: ' + input_sentence
    input_ids = t5_tokenizer.encode(input_text, return_tensors='pt').to(device)
    outputs = t5_small_dropout_model.generate(input_ids=input_ids, max_length=75, num_beams=5, early_stopping=True)
    generated_ending = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Split the output text into sentences and pick the first sentence
    output_sentence = generated_ending.split(".")[0]

    # Compute the BLEURT score
    bleurt_score = bleurt_scorer.score(references=[output_sentence], candidates=[actual_ending])[0]
    scores.append(bleurt_score)

    # Print the input sentence, actual ending, and generated ending
    # print('Input sentence:', input_sentence)
    # print('Actual ending:', actual_ending)
    # print('Generated ending:', output_sentence)
    # print()

# Compute the average BLEURT score
avg_bleurt_score = sum(scores) / len(scores)
print("Average BLEURT score:", avg_bleurt_score)

!pip install rouge

from rouge import Rouge
rouge = Rouge()

x_test_sub = df_test['X'].astype(str).head(100).values.tolist()
y_test_sub = df_test['sentence5'].astype(str).head(100).values.tolist()

# Define lists to store references and hypothesis
references = []
hypotheses = []

# Generate story endings
for input_sentence, actual_ending in zip(x_test_sub, y_test_sub):
    input_text = "Complete the story: " + input_sentence
    input_ids = t5_tokenizer.encode(input_text, return_tensors="pt").to(device)
    output_ids = t5_small_model.generate(input_ids=input_ids, max_length=75, num_beams=5, early_stopping=True)
    output_text = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)

    references.append(actual_ending)
    hypotheses.append(output_text)

# Evaluate using ROUGE
scores = rouge.get_scores(hypotheses, references, avg=True)
print(scores)
print()

# Print scores in a prettier format
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

# Define lists to store references and hypothesis
references = []
hypotheses = []

# Generate story endings
for input_sentence, actual_ending in zip(x_test_sub, y_test_sub):
    input_text = "Complete the story: " + input_sentence
    input_ids = t5_tokenizer.encode(input_text, return_tensors="pt").to(device)
    output_ids = t5_small_model_sgd.generate(input_ids=input_ids, max_length=75, num_beams=5, early_stopping=True)
    output_text = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)

    references.append(actual_ending)
    hypotheses.append(output_text)

# Evaluate using ROUGE
scores = rouge.get_scores(hypotheses, references, avg=True)
print(scores)
print()

# Print scores in a prettier format
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

# Define lists to store references and hypothesis
references = []
hypotheses = []

# Generate story endings
for input_sentence, actual_ending in zip(x_test_sub, y_test_sub):
    input_text = "Complete the story: " + input_sentence
    input_ids = t5_tokenizer.encode(input_text, return_tensors="pt").to(device)
    output_ids = t5_small_dropout_model.generate(input_ids=input_ids, max_length=75, num_beams=5, early_stopping=True)
    output_text = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)

    references.append(actual_ending)
    hypotheses.append(output_text)

# Evaluate using ROUGE
scores = rouge.get_scores(hypotheses, references, avg=True)
print(scores)
print()

# Print scores in a prettier format
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

"""#### OPT Decoder

```
Import the Open Pre-Transformer Tokenizer and Model from Hugging Face
```
"""

opt_tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m", padding_side='left', use_fast = False)
opt_model = TFOPTForCausalLM.from_pretrained("facebook/opt-350m")

"""### Evaluate 6 OPT Models

```
# Create the 6 permutations of the OPT 350M parameter model that we want to evaluate
#1: Greedy Search
#2: Beam Search with 3 beams + no_repeat_ngram_size = 2
#3: Beam Search with top_k
#4: Beam Search using top_k + temperature = 0.7
#5: Top_k = 50
#6: Top-p = 0.92

#Run the x_test set through each model to generate text
```
"""

# Model #1: Greedy Search
#outputs_greedy = opt_model.generate(input_ex, max_length = 75)
#strs = opt_tokenizer.decode(outputs_greedy[0], skip_special_tokens = True)

output_sentences_1 = []

with tf.device('/device:GPU:0'):
  for i in range(100):
    input_ex_1 = opt_tokenizer.encode(x_test[i], return_tensors = 'tf')
    outputs_greedy = opt_model.generate(input_ex_1, max_length = 75)
    strs = opt_tokenizer.decode(outputs_greedy[0], skip_special_tokens = True)
    output_sentences_1.append(strs)

output_sentences_1

output_sentences_1[1]

import csv


with open('/content/gdrive/My Drive/file_model_1_greedy_search.csv', 'w', newline='') as myfile:
     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)
     wr.writerow(output_sentences_1)

x_test[1]

y_test[1]

# Model #2: Beam Search with 3 beams + no_repeat_ngram_size = 2

output_sentences_2 = []

with tf.device('/device:GPU:0'):
  for i in range(100):
    input_ex_1 = opt_tokenizer.encode(x_test[i], return_tensors = 'tf')
    outputs_beam = opt_model.generate(input_ex_1, max_length = 75, num_beams = 3, no_repeat_ngram_size = 2, early_stopping = True)
    strs_2 = opt_tokenizer.decode(outputs_beam[0], skip_special_tokens = True)
    output_sentences_2.append(strs_2)

output_sentences_2[6]

y_test[6]

with open('/content/gdrive/My Drive/file_model_2_beam_search.csv', 'w', newline='') as myfile:
     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)
     wr.writerow(output_sentences_2)

# Model #3: Beam Search using top k

output_sentences_3 = []

with tf.device('/device:GPU:0'):
  for i in range(100):
    input_ex_3 = opt_tokenizer.encode(x_test[i], return_tensors = 'tf')
    outputs_beam2 = opt_model.generate(input_ex_3, do_sample = True, max_length = 75, top_k = 0)
    strs_3 = opt_tokenizer.decode(outputs_beam2[0], skip_special_tokens = True)
    output_sentences_3.append(strs_3)

output_sentences_3[10]

y_test[10]

with open('/content/gdrive/My Drive/file_model_3_beam_search_2.csv', 'w', newline='') as myfile:
     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)
     wr.writerow(output_sentences_3)

# Model #4: Beam search using top_k = 0 + temperature = 0.7

output_sentences_4 = []

with tf.device('/device:GPU:0'):
  for i in range(100):
    input_ex_4 = opt_tokenizer.encode(x_test[i], return_tensors = 'tf')
    outputs_beam3 = opt_model.generate(input_ex_4, do_sample = True, max_length = 75, top_k = 0, temperature = 0.7)
    strs_4 = opt_tokenizer.decode(outputs_beam3[0], skip_special_tokens = True)
    output_sentences_4.append(strs_4)

output_sentences_4[10]

y_test[10]

import csv

with open('/content/gdrive/My Drive/file_model_4_beam_search_3.csv', 'w', newline='') as myfile:
     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)
     wr.writerow(output_sentences_4)

# Model #5: Top_k = 50

output_sentences_5 = []

with tf.device('/device:GPU:0'):
  for i in range(100):
    input_ex_5 = opt_tokenizer.encode(x_test[i], return_tensors = 'tf')
    outputs_top_k = opt_model.generate(input_ex_5, do_sample = True, max_length = 75, top_k = 50)
    strs_5 = opt_tokenizer.decode(outputs_top_k[0], skip_special_tokens = True)
    output_sentences_5.append(strs_5)

output_sentences_5[10]

import csv

with open('/content/gdrive/My Drive/file_model_5_top_k.csv', 'w', newline='') as myfile:
     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)
     wr.writerow(output_sentences_5)

#Model #6: Top_p = 0.92

output_sentences_6 = []

with tf.device('/device:GPU:0'):
  for i in range(100):
    input_ex_6 = opt_tokenizer.encode(x_test[i], return_tensors = 'tf')
    outputs_top_p = opt_model.generate(input_ex_6, do_sample = True, max_length = 75, top_p = 0.92, top_k = 0)
    strs_6 = opt_tokenizer.decode(outputs_top_p[0], skip_special_tokens = True)
    output_sentences_6.append(strs_6)

output_sentences_6[10]

import csv

with open('/content/gdrive/My Drive/file_model_6_top_p.csv', 'w', newline='') as myfile:
     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)
     wr.writerow(output_sentences_6)

"""### Unpack results of 6 Models from stored CSVs + model output data processing

```
Re-load in the stored model outputs from CSV file.
```
"""

import csv
#OPT Model 1 Output
with open('/content/gdrive/My Drive/file_model_1_greedy_search.csv', 'r') as file:
    reader = csv.reader(file)
    opt_model_1_greedy = list(reader)[0]

print(opt_model_1_greedy[0])

#OPT Model 2 Output
with open('/content/gdrive/My Drive/file_model_2_beam_search.csv', 'r') as file:
    reader = csv.reader(file)
    opt_model_2_beam = list(reader)[0]

print(opt_model_2_beam[0])

#OPT Model 3 Output
with open('/content/gdrive/My Drive/file_model_3_beam_search_2.csv', 'r') as file:
    reader = csv.reader(file)
    opt_model_3_beam = list(reader)[0]

print(opt_model_3_beam[0])

#OPT Model 4 Output
with open('/content/gdrive/My Drive/file_model_4_beam_search_3.csv', 'r') as file:
    reader = csv.reader(file)
    opt_model_4_beam = list(reader)[0]

print(opt_model_4_beam[0])

#OPT Model 5 Output
with open('/content/gdrive/My Drive/file_model_5_top_k.csv', 'r') as file:
    reader = csv.reader(file)
    opt_model_5_top_k = list(reader)[0]

print(opt_model_5_top_k[0])

#OPT Model 6 Output
with open('/content/gdrive/My Drive/file_model_6_top_p.csv', 'r') as file:
    reader = csv.reader(file)
    opt_model_6_top_p = list(reader)[0]

print(opt_model_6_top_p[0])

"""

```
Model Output - Data Processing:
Step 1. Remove any text after the 5th sentence is generated
Ste[ 2. Remove the first 4 sentences (ie the context) from each output
```

"""

context = x_test[:100]
print(context)

#Model 1 Data Processing

new_text_model_1 = []

for i in range(100):
  strs = opt_model_1_greedy[i].replace(context[i],"")
  new_text_model_1.append(strs)

sep = '.'
final_new_text_model_1 = []


for i in range(100):
  stripped = new_text_model_1[i].split(sep,1)[0]
  final_new_text_model_1.append(stripped)

print(final_new_text_model_1)

#Model 2 Data Processing

new_text_model_2 = []

for i in range(100):
  strs = opt_model_2_beam[i].replace(context[i],"")
  new_text_model_2.append(strs)

sep = '.'
final_new_text_model_2 = []


for i in range(100):
  stripped = new_text_model_2[i].split(sep,1)[0]
  final_new_text_model_2.append(stripped)

print(final_new_text_model_2)

#Model 3 Data Processing

new_text_model_3 = []

for i in range(100):
  strs = opt_model_3_beam[i].replace(context[i],"")
  new_text_model_3.append(strs)

sep = '.'
final_new_text_model_3 = []


for i in range(100):
  stripped = new_text_model_3[i].split(sep,1)[0]
  final_new_text_model_3.append(stripped)

print(final_new_text_model_3)

#Model 4 Data Processing

new_text_model_4 = []

for i in range(100):
  strs = opt_model_4_beam[i].replace(context[i],"")
  new_text_model_4.append(strs)

sep = '.'
final_new_text_model_4 = []


for i in range(100):
  stripped = new_text_model_4[i].split(sep,1)[0]
  final_new_text_model_4.append(stripped)

print(final_new_text_model_4)

#Model 5 Data Processing

new_text_model_5 = []

for i in range(100):
  strs = opt_model_5_top_k[i].replace(context[i],"")
  new_text_model_5.append(strs)

sep = '.'
final_new_text_model_5 = []


for i in range(100):
  stripped = new_text_model_5[i].split(sep,1)[0]
  final_new_text_model_5.append(stripped)

print(final_new_text_model_5)

#Model 6 Data Processing

new_text_model_6 = []

for i in range(100):
  strs = opt_model_6_top_p[i].replace(context[i],"")
  new_text_model_6.append(strs)

sep = '.'
final_new_text_model_6 = []


for i in range(100):
  stripped = new_text_model_6[i].split(sep,1)[0]
  final_new_text_model_6.append(stripped)

print(final_new_text_model_6)

"""### OPT Model Evaluation for 6 candidate models:
- BLEURT
- ROUGE
"""

sample_y_test = y_test[:100]
print(len(sample_y_test))

#BLEURT SCORES
# Set tf.enable_eager_execution() if using TF 1.x.
import tensorflow as tf
print(tf.__version__)

!pip install --upgrade pip

!pip install git+https://github.com/google-research/bleurt.git

import bleurt
from bleurt import score

# MODEL 1 BLEURT SCORE
references = tf.constant(sample_y_test) #correct results
candidates = tf.constant(final_new_text_model_1) #generated 5th sentence

bleurt_ops = score.create_bleurt_ops()
bleurt_out = bleurt_ops(references=references, candidates=candidates)

assert bleurt_out["predictions"].shape == (100,)
print(bleurt_out["predictions"],'\n')
print("Model 1 Average BLEURT score: ", sum(bleurt_out["predictions"])/100)

# MODEL 2 BLEURT SCORE
references = tf.constant(sample_y_test)
candidates = tf.constant(final_new_text_model_2)

bleurt_ops = score.create_bleurt_ops()
bleurt_out = bleurt_ops(references=references, candidates=candidates)

assert bleurt_out["predictions"].shape == (100,)
print(bleurt_out["predictions"],'\n')
print("Model 2 Average BLEURT score: ", sum(bleurt_out["predictions"])/100)

# MODEL 3 BLEURT SCORE
references = tf.constant(sample_y_test)
candidates = tf.constant(final_new_text_model_3)

bleurt_ops = score.create_bleurt_ops()
bleurt_out = bleurt_ops(references=references, candidates=candidates)

assert bleurt_out["predictions"].shape == (100,)
print(bleurt_out["predictions"],'\n')
print("Model 3 Average BLEURT score: ", sum(bleurt_out["predictions"])/100)

# MODEL 4 BLEURT SCORE
references = tf.constant(sample_y_test)
candidates = tf.constant(final_new_text_model_4)

bleurt_ops = score.create_bleurt_ops()
bleurt_out = bleurt_ops(references=references, candidates=candidates)

assert bleurt_out["predictions"].shape == (100,)
print(bleurt_out["predictions"],'\n')
print("Model 4 Average BLEURT score: ", sum(bleurt_out["predictions"])/100)

# MODEL 5 BLEURT SCORE
references = tf.constant(sample_y_test)
candidates = tf.constant(final_new_text_model_5)

bleurt_ops = score.create_bleurt_ops()
bleurt_out = bleurt_ops(references=references, candidates=candidates)

assert bleurt_out["predictions"].shape == (100,)
print(bleurt_out["predictions"],'\n')
print("Model 5 Average BLEURT score: ", sum(bleurt_out["predictions"])/100)

# MODEL 6 BLEURT SCORE
references = tf.constant(sample_y_test)
candidates = tf.constant(final_new_text_model_6)

bleurt_ops = score.create_bleurt_ops()
bleurt_out = bleurt_ops(references=references, candidates=candidates)

assert bleurt_out["predictions"].shape == (100,)
print(bleurt_out["predictions"],'\n')
print("Model 6 Average BLEURT score: ", sum(bleurt_out["predictions"])/100) #this where avg tensor outputs.

#highest positive float value is best.

# ROUGE SCORES
!pip install rouge
from rouge import Rouge

#Model 1 ROUGE SCORES

rouge = Rouge()

# Generate a set of sentences from your model
generated_sentences = final_new_text_model_1

# Provide a set of reference sentences to compare against
reference_sentences = sample_y_test

# Compute the ROUGE scores
scores = rouge.get_scores(generated_sentences, reference_sentences, avg=True)

# Print scores in a prettier format
print("Model 1 ROUGE scores")
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

#Model 2 ROUGE SCORES

from rouge import Rouge

rouge = Rouge()

# Generate a set of sentences from your model
generated_sentences = final_new_text_model_2

# Provide a set of reference sentences to compare against
reference_sentences = sample_y_test

# Compute the ROUGE scores
scores = rouge.get_scores(generated_sentences, reference_sentences, avg=True, ignore_empty = True)

# Print scores in a prettier format
print("Model 1 ROUGE scores")
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

#Model 3 ROUGE SCORES

rouge = Rouge()

# Generate a set of sentences from your model
generated_sentences = final_new_text_model_3

# Provide a set of reference sentences to compare against
reference_sentences = sample_y_test

# Compute the ROUGE scores
scores = rouge.get_scores(generated_sentences, reference_sentences, avg=True, ignore_empty = True)

# Print scores in a prettier format
print("Model 1 ROUGE scores")
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

#Model 4 ROUGE SCORES

rouge = Rouge()

# Generate a set of sentences from your model
evaluated_sentences = final_new_text_model_4

# Provide a set of reference sentences to compare against
reference_sentences = sample_y_test

# Compute the ROUGE scores
scores = rouge.get_scores(generated_sentences, evaluated_sentences, avg=True, ignore_empty = True)

# Print scores in a prettier format
print("Model 1 ROUGE scores")
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

#Model 5 ROUGE SCORES

rouge = Rouge()

# Generate a set of sentences from your model
generated_sentences = final_new_text_model_5

# Provide a set of reference sentences to compare against
reference_sentences = sample_y_test

# Compute the ROUGE scores
scores = rouge.get_scores(generated_sentences, reference_sentences, avg=True, ignore_empty = True)

# Print scores in a prettier format
print("Model 1 ROUGE scores")
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

#Model 6 ROUGE SCORES

rouge = Rouge()

# Generate a set of sentences from your model
generated_sentences = final_new_text_model_6

# Provide a set of reference sentences to compare against
reference_sentences = sample_y_test

# Compute the ROUGE scores
scores = rouge.get_scores(generated_sentences, reference_sentences, avg=True, ignore_empty = True)

# Print scores in a prettier format
print("Model 1 ROUGE scores")
print("ROUGE-1: ")
print(f"  Precision: {scores['rouge-1']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-1']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-1']['f']*100:.2f}%\n")
print("ROUGE-2: ")
print(f"  Precision: {scores['rouge-2']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-2']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-2']['f']*100:.2f}%\n")
print("ROUGE-L: ")
print(f"  Precision: {scores['rouge-l']['p']*100:.2f}%")
print(f"  Recall: {scores['rouge-l']['r']*100:.2f}%")
print(f"  F1 score: {scores['rouge-l']['f']*100:.2f}%")

"""## Options for batch processing for OPT"""

#Option 1 for batch processing
opt_input_ex = opt_tokenizer.batch_encode_plus(x_test[:100],
              max_length=75,
              truncation=True,
              padding='max_length',
              return_tensors='tf')
opt_input_ex

outputs_greedy = opt_model.generate(**opt_input_ex, max_length = 100)
strs = opt_tokenizer.batch_decode(outputs_greedy, skip_special_tokens = True)

print(strs)